---
title: '[PAPER] DeepETA: How Uber Predicts ETA'
tags: research transformer
---

**[Uber](https://www.uber.com/)** 에서는 **도착 시간 예측**(**Estimating Time of Arrival, ETA**)이 사용자 경험 개선을 위해 매우 중요하다. ETA를 활용하여 요금을 추정하고, 탑승까지 소요되는 시간을 예측하고, 사용자와 운전자를 매칭시키며 배달 운송 경로를 계획하는 등의 다양한 기능을 고도화한다. 전통적인 ETA 계산은 routing engine을 활용하였는데, 이는 커다란 도로망을 잘게 분할하고, 도로 상황에 따라 그래프 상 간선에 가중치를 주는 식으로 설계된다. 이후, 최단 경로 알고리즘에 따라 계산된 경로의 가중치를 모두 합하여 ETA를 추정했다. <br>
하지만 이러한 방식은 다음의 문제점을 가지고 있었다.

- 도로망을 묘사한 그래프는 추상적 표현일 뿐이며, 지면 상 실시간 상황/정보를 담기에 불충분하다.
- 사용자와 운전자가 어떠한 경로를 택할 것인지 예측이 불가하며, 경로 이동 도중에 변경될 여지가 있다.

그에 따라 Uber는 machine learning(ML) 모델을 도입하여 개선하고자 수년간 노력했다. 초기에는 gradient-boosted decision tree ensembles를 이용해 ETA 문제를 풀고자 했으나, 데이터와 모델의 크기가 점점 방대해지는 난관에 봉착했다. 이후, 크키가 큰 데이터에 용이한 deep learning 방식에 관심을 가지기 시작했고 다음의 3가지 주요한 목표(challenges)를 달성해야 했다.

- **Latency:** model은 몇 밀리초(millisecond) 내에 ETA 예측을 해야 한다.
- **Accuracy:** 이전 XGBoost model과 비교하여 mean absolute error(MAE)가 눈에 띄게 개선되어야 한다.
- **Generality:** model은 글로벌한 Uber의 수많은 mobility와 delivery 서비스에 활용할 수 있는 ETA 예측을 수행할 수 있어야 한다.

이를 해결하기 위해, Uber AI팀은 Uber Maps팀과 협력하여 **DeepETA**라 불리는 프로젝트를 시작했다. 

## 배경

지난 수년간, 실세계를 표현하는 physical model과 deep learning을 결합한 시스템에 대한 연구는 활발히 이루어지고 있다. 본 기술에서는 physical model을 담당하는 routing engine은 지도의 데이터와 실시간 교통 정보를 계산한다. 그 후, machine learning model은 routing engine 상에서의 ETA와 실세계에서의 관측치의 잔차(residual)를 예측한다. 이러한 접근 방식을 hybrid approach ETA post-processing이라 부르며, DeepETA는 해당 방식에서의 machine learning model이다. 실용적 관점에서 보면, 새로운 유형의 데이터와 요구사항이 쏟아질 때에 routing engine 자체를 뜯어고치는 것보다 post-processing model을 수정하는 것이 용이하다.

<center><img src="https://github.com/hongjun7/log/blob/main/assets/2023-01-22/01.png?raw=true" width="600"></center>

post-processing 모델은 그림과 같이 출발지와 목적지 및 시간과 같은 시공간적 특징과 실시간 트래픽에 대한 정보, 그리고 요청의 특성(delivery dropoff, rideshare pickup 등)을 고려한다. 이 모델은 Uber에서 가장 높은 QPS(queries per second) 성능을 가진 모델이다. ETA 요청에 대해 과도한 latency가 부가되지 않도록 빨라야하며, 데이터의 여러 segment에 걸쳐 MAE를 기준으로 측정된 ETA 정확도가 향상되어야 한다.

## 정확도 개선 방법

DeepETA 연구팀은 7종([MLP](https://en.wikipedia.org/wiki/Multilayer_perceptron), [NODE](https://arxiv.org/pdf/1909.06312.pdf), [TabNet](https://www.aaai.org/AAAI21Papers/AAAI-1063.ArikS.pdf), [Sparsely Gated Mixture-of-Experts](https://arxiv.org/pdf/1701.06538.pdf), [HyperNetworks](https://arxiv.org/pdf/1609.09106.pdf), [Transformer](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) and [Linear Transformer](https://arxiv.org/pdf/2006.16236.pdf))에 대해 실험하고 튜닝했다. 그 결과, self-attention 기반의 encoder 즉, Transformer가 가장 높은 accuracy를 보여주었다. 아래 그림은 DeepETA architecture를 간소화한 것으로, 다양한 feature encoding을 대상으로 실험해보았을 때에 이산화(discretizing)하여 모든 입력값을 embedding했을 때에 다른 기법들보다 훨씬 나았다.

<center><img src="https://github.com/hongjun7/log/blob/main/assets/2023-01-22/02.png?raw=true" width="600"></center>

### Self-Attention 기반의 Encoder

[Transformer](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) 구조는 self-attention 기반의 신경망 구조로 정의되고, self-atteiontion은 벡터들의 수열을 입력값으로 받아서 가중된(weighted) 벡터들의 수열을 출력하는 sequence-to-sequence 연산이다. 그래서 Transformer를 ETA 예측과 같은 tabular 데이터 문제에 적용하는 것은 생소할 수 있다. <br>
언어 모델에서 각 벡터는 single word token을 나타내지만, DeepETA에서는 single feature(출발 위치, 출발 시각 등)를 나타낸다. self-attention은 feature의 총 개수 $K$개에 대해 $K \times K$ attention matrix을 계산하여 feature간의 pairwise 상호작용을 추정한다. self-attention layer는 각 feature를 처리할 때 input의 다른 모든 feature의 가중 합계로 표현하여 시간적, 공간적 특징에 대한 이해를 반영한다. 언어 모델과 대조적으로, DeepETA에서는 feature 간 순서는 중요하지 않기 때문에 위치 encoding 과정이 없다. 

<center><img src="https://github.com/hongjun7/log/blob/main/assets/2023-01-22/03.png?raw=true" width="600"></center>

출발지 A에서 목적지 B로의 이동을 예로 들면, self-attention layer는 시간, 출발지와 목적지, 교통 상황 등에 따라 feature의 중요도를 산정(scaling)한다. 아래 그림은 8개의 attention-head에 해당하는 8가지 색상과 랜덤하게 생성된 attention weight의 공유 상황을 나타낸다.

<center><img src="https://github.com/hongjun7/log/blob/main/assets/2023-01-22/04.gif?raw=true" width="600"></center>

### Feature Encoding

#### Continuous and Categorical Features

DeepETA 모델은 모든 categorical feature를 embedding하고, 모든 **continuous featrue를 embedding하기 전에 버킷화**(**bucketize**)합니다. 예상과는 달리, continuous feature를 버킷화하면 직접 사용하는 것보다 정확도가 향상되었다. 신경망이 비선형 불연속 함수를 학습할 수 있기 때문에 버킷화가 반드시 필요하지는 않지만, 버킷화된 기능을 가진 신경망은 input space를 분할하기 위해 parameter 할당을 전혀 할 필요가 없기 때문에 이점이 있었을 것이라 추정한다. <br>
[Gradient Boosted Decision Tree Neural Network](https://arxiv.org/pdf/1910.09340.pdf)와 마찬가지로 quantile bucket을 사용하면 equal-width bucket보다 더 나은 정확도를 제공한다는 것을 발견했다. 그리고 그 이유로는 해당 방식이 다른 방식보다 엔트로피를 최대화하기 때문이라고 추정한다.

#### Geospatial Embeddings

post-processing model은 출발지와 목적지를 위도와 경도로 입력 받는다. 경로의 시작과 끝은 ETA 예측에 매우 중요하기 때문에 DeepETA는 다른 continuous featrue와는 다르게 인코딩한다. 위치 데이터는 전세계에 매우 불균일하게 분포하며 복잡한 공간 해상도를 지닌다. 따라서 위도와 경도 정보에 따라 특정 위치를 여러 해상도의 격자로 분할(quantization)한다. 해상도가 증가함에 따라 구별되는 격자의 칸 수는 기하급수적으로 증가하고 각 격자 칸의 평균 데이터 양은 비례적으로 감소한다. 이러한 격자 구조를 embedding에 매핑하기 위한 아래 3가지 전략을 사용한다.

- **Exact indexing**, which maps each grid cell to a dedicated embedding.
- **Feature hashing**, which maps each grid cell into a compact range of bins using a hash function.
- **Multiple feature hashing**, which extends feature hashing by mapping each grid cell to multiple compact ranges of bins using independent hash functions.

<center><img src="https://github.com/hongjun7/log/blob/main/assets/2023-01-22/05.png?raw=true" width="600"></center>

실험 결과, Multiple feature hashing은 정확한 인덱싱에 비해 공간을 절약하면서도 최상의 정확도와 대기 시간을 제공했다. 이는 신경망이 여러 개의 독립적인 hash bucket의 정보를 결합하여 single-bucket hash collision의 부정적인 효과를 되돌릴 수 있음을 의미한다.

## 속도 개선 방법

### Linear Transformer

Transformer 기반 encoder가 최고의 정확도를 보여주었지만, 온라인 실시간 배포를 위한 대기 시간 요청 제약을 충족하기에는 속도가 너무 느렸다. 기본적으로 self-attention model은 $K$개의 입력에 대해 attention matrix를 계산하느라 &K^2$의 복잡도를 가진다. 이러한 self-attention 계산을 선형화하기 위한 연구가 여럿 있었는데, 예를 들면 [Linear Transformer](https://arxiv.org/pdf/2006.16236.pdf), [Linformer](https://arxiv.org/pdf/2006.04768.pdf), [Performer](https://arxiv.org/pdf/2009.14794.pdf)가 있다. 실험 결과, kernel trick을 이용해 attention matrix 계산을 회피하는 Linear Transformer가 가장 준수한 성능을 보여주었다.

### More Embeddings, Fewer Layers

DeepETA를 빠르게 만드는 또 다른 요소는 feature의 희소성(sparsity)을 활용하는 것이다. 이 모형은 수억 개의 parameter를 가지고 있지만, 특정 단일 inference은 전체 parameter의 약 0.25%에 해당하는 아주 작은 부분만 건드린다. 모형 자체는 적은 수의 layer만으로 구성되어 있다. 그리고 대부분의 parameter는 embedding lookup table에 위치한다. 입력을 이산화하고 embedding에 매핑함으로써, 사용된 적 없는 parameter의 연산을 회피한다.

## 일반화 방법

DeepETA의 설계 목표 중 하나는 전세계 Uber의 모든 사업부에 서비스를 제공하는 일반화된 모델을 제공하는 것이다. 이는 사업부마다 요구사항이 다르고 데이터 배포가 다르기 때문에 어려운 문제이다. 전체 모델의 개괄적인 구조는 아래 그림과 같다.

<center><img src="https://github.com/hongjun7/log/blob/main/assets/2023-01-22/06.png?raw=true" width="1000"></center>

### Bias Adjustment Decoder

효과적인 feature representation을 수행했다면, 다음 단계는 그것을 decoding하고 예측하는 것이다. decoder는 segment bias adjustment layer가 포함된 fully connected 신경망이다. 다양한 segment 각각에 대한 raw prediction을 조정하기 위해 bias adjustment layer를 추가하는 것은, segment의 variation에 대응하여 MAE를 개선할 수 있다. 이 접근법은 단순히 segment feature를 모델에 추가하는 것보다 효과적이다. multi-task decoder 대신 bias adjustment layer를 구현한 이유는 latency 제약 때문이다. 또한 예측 정확도를 더 향상시키기 위해 몇 가지 기법을 사용했는데, 예측된 ETA가 양수가 되도록 강제하기 위해 출력에서 ReLU를 사용하는 것과 극단적인 값의 효과를 줄이기 위해 clamping하는 것을 예로 들 수 있다.

### Asymmetric Huber Loss

DeepETA는 파라미터화된 손실 함수인 Asymmetric Huber Loss을 사용하는데, 이는 특이치에 강건하며 일반적으로 사용되는 다양한 점 추정치(point estimates)를 계산하는데 활용된다. <br> Asymmetric Huber Loss은 $\delta$와 $\omega$ 두 가지 파라미터를 가지며, 이는 각각 특이치에 대한 강건성의 정도와 비대칭성의 정도를 제어한다. $\delta$를 변화시킴으로써, 제곱 오차와 절대 오차를 원활하게 보간할 수 있으며, 후자는 특이치에 덜 민감하다. $\omega$를 달리하면 과소예측 대 과잉예측의 상대적인 비용을 조절할 수 있어 1분 늦는 것이 1분 일찍 하는 것보다 더 나쁜 상황에서 유용하다. 이러한 모수는 일반적으로 사용되는 다른 회귀 손실 함수를 모방할 수 있을 뿐만 아니라 모형이 생산하는 점 추정치를 다양한 비즈니스 목표에 맞게 활용할 수 있도록 도와준다.

<center><img src="https://github.com/hongjun7/log/blob/main/assets/2023-01-22/07.gif?raw=true" width="600"></center>

## 결론

종래의 XGBoost 기반 접근 방식과 비교해보았을 때에 DeepETA 모델은 우수한 성능의 대규모 모델을 효율적으로 학습시켰고 상용화에 성공했다. DeepETA는 production-metric을 실시간으로 개선하고, 여러 소비자 사용 사례에 재사용할 수 있는 모델을 제공한다. <br>
DeepETA를 통해 다양한 모델 구조를 탐색하여 소비자 개개인에 알맞게 조정할 수 있는 여러 종류의 ETA를 추정할 수 있다. 추후 Uber AI는 Dataset, Features, Transformation, Model Architecture, Training Algorithm, Loss Functions 및 Infrastructure 개선을 포함하는 모든 모델링 프로세스를 최적화하여 향상된 성능을 기대할 수 있다.

## References
- [Wu et al. 2019. DeepETA: A Spatial-Temporal Sequential Neural Network Model for Estimating Time of Arrival in Package Delivery System](https://ojs.aaai.org/index.php/AAAI/article/view/3856)
- [Hu et al. 2022. DeeprETA: An ETA Post-processing System at Scale ](https://arxiv.org/pdf/2206.02127.pdf)
- [Uber Blog: DeepETA: How Uber Predicts Arrival Times Using Deep Learning](https://www.uber.com/blog/deepeta-how-uber-predicts-arrival-times/)
- [Uber Blog: Meet Michelangelo: Uber’s Machine Learning Platform](https://www.uber.com/blog/michelangelo-machine-learning-platform/)
- [Saberian et al. 2019. Gradient Boosted Decision Tree Neural Network](https://arxiv.org/pdf/1910.09340.pdf)